{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CoronaBasic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htDHburyBuEe",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary Analysis of CoronaVirus Time Series Data\n",
        "In this notebook we will conduct some preliminary analysis and forecasting on the Coronavirus time seires data. For this analysis we will look at \n",
        "\n",
        "**Warning this is a basic analysis/machine learning model. The goal of this notebook is to gage the utility of data augmentation/transfer learning for virus forecasting. NOT provide actionable insights. It would additional rounds of training/validation + verification by epidemiologists and public health experts before I would be confident relying on using it for any actionable insights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA1NxnWmHOAQ",
        "colab_type": "code",
        "outputId": "8cfe5d21-4da3-4710-cc73-9d402fad5337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#!git clone https://github.com/CoronaWhy/task-geo.git\n",
        "#os.chdir('task-geo')\n",
        "#make install\n",
        "import pandas as pd\n",
        "!wget -O coronavirus_timeseries.csv https://coronadatascraper.com/timeseries.csv\n",
        "!pip install wandb\n",
        "!wandb login\n",
        "import wandb\n",
        "from tensorflow import keras\n",
        "from wandb.keras import WandbCallback "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-26 15:10:23--  https://coronadatascraper.com/timeseries.csv\n",
            "Resolving coronadatascraper.com (coronadatascraper.com)... 185.199.110.153, 185.199.111.153, 185.199.108.153\n",
            "Connecting to coronadatascraper.com (coronadatascraper.com)|185.199.110.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34699757 (33M) [text/csv]\n",
            "Saving to: ‘coronavirus_timeseries.csv’\n",
            "\n",
            "coronavirus_timeser 100%[===================>]  33.09M  42.3MB/s    in 0.8s    \n",
            "\n",
            "2020-04-26 15:10:23 (42.3 MB/s) - ‘coronavirus_timeseries.csv’ saved [34699757/34699757]\n",
            "\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/30/cb8f29239f638cc2b03ac25642b78ced1e927c448a1e16a4a7f566a29436/wandb-0.8.33-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
            "\u001b[?25hCollecting gql==0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.352.0)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/7e/19545324e83db4522b885808cd913c3b93ecc0c88b03e037b78c6a417fa8/sentry_sdk-0.14.3-py2.py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 20.5MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.0MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/6b/01baa293090240cf0562cc5eccb69c6f5006282127f2b846fad011305c79/configparser-5.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.12.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.1)\n",
            "Collecting watchdog>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz (95kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.6MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/1a/0df85d2bddbca33665d2148173d3281b290ac054b5f50163ea735740ac7b/GitPython-3.1.1-py3-none-any.whl (450kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 16.3MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting graphql-core<2,>=0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz (70kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from gql==0.2.0->wandb) (2.3)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.4.5.1)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/52/ca35448b56c53a079d3ffe18b1978c6e424f6d4df02404877094c89f5bfb/gitdb-4.0.4-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->wandb) (2.8)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/27/b1/e379cfb7c07bbf8faee29c4a1a2469dbea525f047c2b454c4afdefa20a30/smmap-3.0.2-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: gql, subprocess32, watchdog, graphql-core, pathtools\n",
            "  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gql: filename=gql-0.2.0-cp36-none-any.whl size=7630 sha256=07e817c84036c170bea6d778f08cf285213f410a702d7709c3ab9b5e19d5142f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=7293a4346cf2179afb40bab28fe729186f4f31587138795eec443e66951d75f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.2-cp36-none-any.whl size=73605 sha256=1fca97b8129e2630b267ae7f34812b07f374e54dbf7f66c93ebd00dd4f6c3639\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for graphql-core: filename=graphql_core-1.1-cp36-none-any.whl size=104650 sha256=78180f92aecf3a944e2b58d18a907271a68736380ebc08f2a432e31042418e24\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8784 sha256=540212d95cfa334c552fee33f1889c207ab620ea25c1c0845973830d3c8a12f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built gql subprocess32 watchdog graphql-core pathtools\n",
            "Installing collected packages: graphql-core, gql, sentry-sdk, subprocess32, docker-pycreds, configparser, pathtools, watchdog, smmap, gitdb, GitPython, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.1 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.4 gql-0.2.0 graphql-core-1.1 pathtools-0.1.2 sentry-sdk-0.14.3 shortuuid-1.0.1 smmap-3.0.2 subprocess32-3.5.4 wandb-0.8.33 watchdog-0.10.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://app.wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: 4c616b51e6e88012c20dc6adcf90d05172185490\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmOWKLuxHT7j",
        "colab_type": "code",
        "outputId": "45e127a3-84c6-406b-c8e4-c3719d999263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "df = pd.read_csv(\"coronavirus_timeseries.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asrssixL6XZo",
        "colab_type": "code",
        "outputId": "b64449a3-ff94-48ae-e3f5-02128b8e2ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "unqiue_counties = df['county'].unique()\n",
        "print(len(unqiue_counties))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc189EXg6pfE",
        "colab_type": "text"
      },
      "source": [
        "## Forecasting in Antwerp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiJyk9YiHaOX",
        "colab_type": "code",
        "outputId": "eab1e303-d9cc-46af-ede5-271430649566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "antwerp_df = df[df['county']=='Antwerp'].fillna(0)\n",
        "antwerp_relevant = antwerp_df[['cases', 'deaths', 'recovered', 'population', 'lat', 'long']].values\n",
        "#antwerp_df.tail()\n",
        "print(len(antwerp_df))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlVWneL_CV3r",
        "colab_type": "text"
      },
      "source": [
        "## Data Augmentation\n",
        "We will now explore using the tsaug library for forecasting.. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1J98G13HbKM",
        "colab_type": "code",
        "outputId": "8d19cb85-b098-4cab-b433-27fcaa8a620e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "!pip install tsaug"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tsaug\n",
            "  Downloading https://files.pythonhosted.org/packages/e8/6e/8b1be145a32bba360c14322c3b87ad93d6227c46528d482c84eefe54094b/tsaug-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from tsaug) (1.18.3)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.6/dist-packages (from tsaug) (1.4.1)\n",
            "Installing collected packages: tsaug\n",
            "Successfully installed tsaug-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXNkSdTFvZ1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tsaug.visualization import plot \n",
        "from tsaug import TimeWarp, Crop, Quantize, Drift, Reverse\n",
        "my_augmenter = (TimeWarp() * 5, # random time warping 5 times in parallel \n",
        "                Crop(size=300),  # random crop subsequences with length 300\n",
        "                Quantize(n_levels=[10, 20, 30]),  # random quantize to 10-, 20-, or 30- level sets\n",
        "                Drift(max_drift=(0.1, 0.5)),   # with 80% probability, random drift the signal up to 10% - 50%\n",
        "                Reverse()) #0.5  # with 50% probability, reverse the sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdVWNhk2XRJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_aug = my_augmenter[0].augment(antwerp_relevant)\n",
        "print(antwerp_relevant.shape)\n",
        "X_aug = TimeWarp(antwerp[:70])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A6ICggnYBlX",
        "colab_type": "text"
      },
      "source": [
        "## Models and Forecasting\n",
        "We will now define some simple models in Keras for forecasting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VO7ZOB41az1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "scaler_dict = {}\n",
        "config_default = {\"epochs\":30, \"validation_split\":0.1, \n",
        "          \"loss\":\"mean_squared_error\", \"optimizer\":'adam', \n",
        "          \"geo_segment\":\"antwerp\", \"seq_len\":7, \"train_steps\":70, \n",
        "          \"test_steps\":27, \"scaler\":\"RobustScaler\", \n",
        "          \"beta\":0.899}\n",
        "r = RobustScaler()\n",
        "x_train_full = antwerp_df[['deaths', 'cases']][:config_default[\"train_steps\"]]\n",
        "x_train_full = pd.DataFrame(r.fit_transform(x_train_full))\n",
        "y_train_full = x_train_full\n",
        "r_test = RobustScaler()\n",
        "test_orig = antwerp_df[['deaths', 'cases']][70:]\n",
        "test = pd.DataFrame(r_test.fit_transform(test_orig))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUR6eM4MZZJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(X, y, time_steps=1):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - time_steps):\n",
        "        v = X.iloc[i:(i + time_steps)].values\n",
        "        Xs.append(v)\n",
        "        ys.append(y.iloc[i + time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "X_train, Y_train = create_dataset(x_train_full, y_train_full, config_default[\"seq_len\"])\n",
        "X_test, y_test = create_dataset(test, test, config_default[\"seq_len\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OOG9RAp4-ap",
        "colab_type": "code",
        "outputId": "d37f1d49-1f45-45dc-ea11-d21c5e1de166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "sweep_config = {\n",
        "  \"name\": \"Default sweep\",\n",
        "  \"method\": \"grid\",\n",
        "  \"parameters\": {\n",
        "        \"batch_size\": {\n",
        "            \"values\": [2, 3, 4, 5]\n",
        "        },\n",
        "        \"learn\":{\n",
        "            \"values\":[0.001, 0.0015, 0.002, 0.003, 0.004, 0.01]\n",
        "        } \n",
        "    }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: sz2r7l4w\n",
            "Sweep URL: https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXhSxkqdYJbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  run = wandb.init(project=\"covid-forecast\", config=config_default, magic=True)\n",
        "  config = wandb.config\n",
        "  opt = keras.optimizers.Adam(learning_rate=config[\"learn\"], beta_1=config[\"beta\"], beta_2=0.999, amsgrad=False)\n",
        "  model = keras.Sequential()\n",
        "  model.add(\n",
        "    keras.layers.Bidirectional(\n",
        "      keras.layers.LSTM(\n",
        "        units=128,\n",
        "        input_shape=(X_train.shape[1], X_train.shape[2])\n",
        "      )\n",
        "    )\n",
        "  ) \n",
        "  model.add(keras.layers.Dropout(rate=0.2))\n",
        "  model.add(keras.layers.Dense(units=2))\n",
        "  model.compile(loss=config[\"loss\"], optimizer=opt)\n",
        "\n",
        "  history = model.fit(\n",
        "      X_train, Y_train,\n",
        "      epochs=config[\"epochs\"],\n",
        "      batch_size=config[\"batch_size\"],\n",
        "      validation_split=config[\"validation_split\"],\n",
        "      callbacks=[WandbCallback()],\n",
        "      shuffle=False\n",
        "  )\n",
        "  evaluate_single(model, X_test, y_test, r)\n",
        "  evaluate_plot_multi(model, test, config, X_test, r_test)\n",
        "  return model\n",
        "\n",
        "def evaluate_single(model, x_test, y_test, scaler):\n",
        "  y_preds = model.predict(x_test)\n",
        "  y_preds = scaler.inverse_transform(y_preds)\n",
        "  y_test = scaler.inverse_transform(y_test)\n",
        "  complete_mse = tf.keras.losses.MSE( y_preds[:, 1], y_test[:, 1])\n",
        "  wandb.run.summary[\"test_mse\"] = complete_mse\n",
        "  return complete_mse\n",
        "\n",
        "def evaluate_plot_multi(model, test_df, config, x_test, scaler):\n",
        "  arr = predict_multi(model, len(test)-config[\"seq_len\"], x_test[0, :, :])\n",
        "  test_orig['predicted_cases'] = 0\n",
        "  test_orig['predicted_cases'][config[\"seq_len\"]:] = scaler.inverse_transform(arr.squeeze(0))[:, 1]\n",
        "  plt.plot(test_orig['predicted_cases'], label='predicted_cases')\n",
        "  plt.plot(test_orig['cases'], label='actual_cases')\n",
        "  plt.legend();\n",
        "  wandb.log({\"test\":plt})\n",
        "  large_mse = tf.keras.losses.MSE(\n",
        "    y_multi[:, 1], test_orig['predicted_cases'][config[\"seq_len\"]:].values\n",
        "  )\n",
        "  wandb.run.summary[\"test_mse_full\"] =  large_mse\n",
        "  return large_mse\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0N5tLiZB_IE",
        "colab_type": "code",
        "outputId": "cbca51a1-6556-49cc-d222-a96d1d007d5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "wandb.agent(sweep_id, function=train)\n",
        "#train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wandb: Agent Starting Run: zf1811zd with config:\n",
            "\tbatch_size: 2\n",
            "\tlearn: 0.004\n",
            "wandb: Agent Started Run: zf1811zd\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/zf1811zd\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/zf1811zd</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "28/28 [==============================] - 1s 33ms/step - loss: 0.0335 - val_loss: 4.7072\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1379 - val_loss: 2.1729\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0574 - val_loss: 7.6313\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1322 - val_loss: 3.6609\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0531 - val_loss: 4.7727\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.4594 - val_loss: 13.5437\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0970 - val_loss: 6.8864\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1104 - val_loss: 3.5850\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 0s 10ms/step - loss: 0.2329 - val_loss: 4.9302\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1374 - val_loss: 9.7213\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.4611 - val_loss: 8.0275\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2522 - val_loss: 5.4802\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2092 - val_loss: 2.4380\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.2054 - val_loss: 4.1080\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.5280 - val_loss: 21.6276\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2297 - val_loss: 9.5466\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.1892 - val_loss: 7.5071\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1987 - val_loss: 9.6216\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.1423 - val_loss: 6.7246\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.2317 - val_loss: 12.6821\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0652 - val_loss: 6.8242\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.0487 - val_loss: 5.8640\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0447 - val_loss: 7.4421\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.0370 - val_loss: 6.2940\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0402 - val_loss: 5.8235\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0302 - val_loss: 6.1548\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0310 - val_loss: 4.9932\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.0195 - val_loss: 5.9962\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0237 - val_loss: 4.9807\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0281 - val_loss: 5.5277\n",
            "wandb: Agent Finished Run: zf1811zd \n",
            "\n",
            "wandb: Agent Starting Run: bikaxo5m with config:\n",
            "\tbatch_size: 2\n",
            "\tlearn: 0.01\n",
            "wandb: Agent Started Run: bikaxo5m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/bikaxo5m\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/bikaxo5m</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "28/28 [==============================] - 1s 35ms/step - loss: 0.0371 - val_loss: 1.4206\n",
            "Epoch 2/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1506 - val_loss: 6.6646\n",
            "Epoch 3/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.2674 - val_loss: 5.6824\n",
            "Epoch 4/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.7094 - val_loss: 9.4924\n",
            "Epoch 5/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2113 - val_loss: 4.3793\n",
            "Epoch 6/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.5281 - val_loss: 0.9074\n",
            "Epoch 7/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2267 - val_loss: 6.6093\n",
            "Epoch 8/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.3561 - val_loss: 9.7046\n",
            "Epoch 9/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2979 - val_loss: 3.3176\n",
            "Epoch 10/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0867 - val_loss: 1.8698\n",
            "Epoch 11/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.1260 - val_loss: 3.7639\n",
            "Epoch 12/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1292 - val_loss: 3.2556\n",
            "Epoch 13/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0655 - val_loss: 0.8341\n",
            "Epoch 14/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.6502\n",
            "Epoch 15/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0227 - val_loss: 0.7266\n",
            "Epoch 16/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0171 - val_loss: 0.5885\n",
            "Epoch 17/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0453 - val_loss: 0.6961\n",
            "Epoch 18/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1773 - val_loss: 3.7437\n",
            "Epoch 19/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.3737 - val_loss: 13.6416\n",
            "Epoch 20/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.2308 - val_loss: 1.2411\n",
            "Epoch 21/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2405 - val_loss: 3.8587\n",
            "Epoch 22/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2199 - val_loss: 5.6036\n",
            "Epoch 23/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.1393 - val_loss: 0.6611\n",
            "Epoch 24/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.0951 - val_loss: 1.0413\n",
            "Epoch 25/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.1196 - val_loss: 2.0399\n",
            "Epoch 26/30\n",
            "28/28 [==============================] - 0s 9ms/step - loss: 0.0947 - val_loss: 0.4564\n",
            "Epoch 27/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.2841 - val_loss: 1.1433\n",
            "Epoch 28/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.3276 - val_loss: 12.2153\n",
            "Epoch 29/30\n",
            "28/28 [==============================] - 0s 7ms/step - loss: 0.1581 - val_loss: 7.0148\n",
            "Epoch 30/30\n",
            "28/28 [==============================] - 0s 8ms/step - loss: 0.5996 - val_loss: 2.0449\n",
            "wandb: Agent Finished Run: bikaxo5m \n",
            "\n",
            "wandb: Agent Starting Run: 1uzxfj2x with config:\n",
            "\tbatch_size: 3\n",
            "\tlearn: 0.001\n",
            "wandb: Agent Started Run: 1uzxfj2x\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/1uzxfj2x\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/1uzxfj2x</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.7153 - val_loss: 27.9806\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.0659 - val_loss: 7.0848\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.1353 - val_loss: 2.2106\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.2967 - val_loss: 12.8101\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0175 - val_loss: 5.4108\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0584 - val_loss: 3.8062\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1280 - val_loss: 9.6555\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0247 - val_loss: 4.9533\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0213 - val_loss: 5.9922\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0170 - val_loss: 4.6203\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 0.0065 - val_loss: 4.6272\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 5.4969\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0277 - val_loss: 4.4683\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0617 - val_loss: 6.4698\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0386 - val_loss: 3.6758\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0972 - val_loss: 7.8638\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0368 - val_loss: 4.1299\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0390 - val_loss: 7.1111\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0291 - val_loss: 4.5598\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0228 - val_loss: 6.2347\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0129 - val_loss: 4.4397\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0240 - val_loss: 5.9021\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0137 - val_loss: 4.4783\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0296 - val_loss: 6.3817\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0218 - val_loss: 4.0562\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0239 - val_loss: 5.7268\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0271 - val_loss: 3.9752\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0354 - val_loss: 5.8101\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0368 - val_loss: 3.5941\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0847 - val_loss: 6.6400\n",
            "wandb: Agent Finished Run: 1uzxfj2x \n",
            "\n",
            "wandb: Agent Starting Run: ne7u8crh with config:\n",
            "\tbatch_size: 3\n",
            "\tlearn: 0.0015\n",
            "wandb: Agent Started Run: ne7u8crh\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/ne7u8crh\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/ne7u8crh</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.5047 - val_loss: 21.8142\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.0563 - val_loss: 4.0202\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1321 - val_loss: 7.5540\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.1917 - val_loss: 3.5322\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.3889 - val_loss: 18.6477\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 0.1210 - val_loss: 11.8603\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0309 - val_loss: 6.4671\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0093 - val_loss: 6.7598\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0202 - val_loss: 6.4970\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0420 - val_loss: 7.4062\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 0.0659 - val_loss: 5.7757\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1040 - val_loss: 9.3792\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0311 - val_loss: 5.4506\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0606 - val_loss: 7.3381\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0496 - val_loss: 4.7236\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0699 - val_loss: 7.8865\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0388 - val_loss: 4.8124\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0455 - val_loss: 7.0849\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0443 - val_loss: 4.8467\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1072 - val_loss: 9.2257\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0273 - val_loss: 5.2754\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0280 - val_loss: 7.1114\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0146 - val_loss: 4.8556\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 5.5302\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 5.1907\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0149 - val_loss: 4.4016\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0090 - val_loss: 4.2541\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 4.6355\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0120 - val_loss: 4.1708\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0128 - val_loss: 4.1859\n",
            "wandb: Agent Finished Run: ne7u8crh \n",
            "\n",
            "wandb: Agent Starting Run: aasiichx with config:\n",
            "\tbatch_size: 3\n",
            "\tlearn: 0.002\n",
            "wandb: Agent Started Run: aasiichx\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/aasiichx\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/aasiichx</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.3354 - val_loss: 12.7544\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1879 - val_loss: 2.2984\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.8728 - val_loss: 30.8242\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.2668 - val_loss: 15.4696\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0575 - val_loss: 5.9968\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.0839 - val_loss: 6.8091\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1298 - val_loss: 6.2294\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0530 - val_loss: 5.9376\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0186 - val_loss: 4.3533\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0563 - val_loss: 6.2870\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 0.1326 - val_loss: 4.2522\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 0.4650 - val_loss: 19.6231\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1435 - val_loss: 12.6760\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0317 - val_loss: 6.3787\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0104 - val_loss: 6.7800\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0214 - val_loss: 6.4105\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 5.7827\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 5.5620\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0144 - val_loss: 5.0524\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0145 - val_loss: 5.4152\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0225 - val_loss: 4.0417\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0500 - val_loss: 6.4513\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0490 - val_loss: 3.8604\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0866 - val_loss: 6.7241\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0495 - val_loss: 3.6842\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.0444 - val_loss: 6.9609\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0307 - val_loss: 4.3415\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0295 - val_loss: 5.4212\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0362 - val_loss: 3.8581\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0921 - val_loss: 7.9657\n",
            "wandb: Agent Finished Run: aasiichx \n",
            "\n",
            "wandb: Agent Starting Run: 828sq9mr with config:\n",
            "\tbatch_size: 3\n",
            "\tlearn: 0.003\n",
            "wandb: Agent Started Run: 828sq9mr\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/828sq9mr\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/828sq9mr</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.1364 - val_loss: 3.9718\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.4265 - val_loss: 2.6598\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.9670 - val_loss: 20.4074\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0372 - val_loss: 6.7785\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.1639 - val_loss: 6.4864\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.1920 - val_loss: 9.3274\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0878 - val_loss: 4.9462\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.3502 - val_loss: 14.4437\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0193 - val_loss: 6.5644\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0210 - val_loss: 5.7496\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 13ms/step - loss: 0.0221 - val_loss: 6.0285\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0490 - val_loss: 4.6202\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1621 - val_loss: 11.0447\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0373 - val_loss: 5.7090\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0416 - val_loss: 7.5352\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0444 - val_loss: 5.3040\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1335 - val_loss: 9.6106\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0523 - val_loss: 5.1019\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0418 - val_loss: 7.3014\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0255 - val_loss: 5.0702\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 5.5518\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0202 - val_loss: 5.6246\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0276 - val_loss: 4.3729\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0170 - val_loss: 3.6907\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0350 - val_loss: 5.3013\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0618 - val_loss: 3.6988\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1887 - val_loss: 10.2088\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0254 - val_loss: 5.3444\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0521 - val_loss: 7.4381\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0400 - val_loss: 4.6319\n",
            "wandb: Agent Finished Run: 828sq9mr \n",
            "\n",
            "wandb: Agent Starting Run: 5zldaje8 with config:\n",
            "\tbatch_size: 3\n",
            "\tlearn: 0.004\n",
            "wandb: Agent Started Run: 5zldaje8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/5zldaje8\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/5zldaje8</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "19/19 [==============================] - 1s 48ms/step - loss: 0.0549 - val_loss: 5.8718\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0753 - val_loss: 2.2201\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.1377 - val_loss: 1.8927\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.5797 - val_loss: 14.4009\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1509 - val_loss: 5.9510\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.2624 - val_loss: 4.6146\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0692 - val_loss: 1.7508\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0651 - val_loss: 1.7730\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1570 - val_loss: 3.8379\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.7390 - val_loss: 28.2694\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 13ms/step - loss: 0.2101 - val_loss: 8.7613\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.7183 - val_loss: 8.8149\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.7063 - val_loss: 30.9361\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.5804 - val_loss: 17.4256\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1477 - val_loss: 5.1570\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0535 - val_loss: 4.4159\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1977 - val_loss: 5.1085\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.5904 - val_loss: 23.6861\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.2824 - val_loss: 14.2590\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1014 - val_loss: 5.9791\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1436 - val_loss: 9.3849\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1207 - val_loss: 5.5441\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.2124 - val_loss: 12.8298\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 7.4162\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0445 - val_loss: 5.8727\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0152 - val_loss: 6.9895\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0259 - val_loss: 5.8712\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0188 - val_loss: 6.1569\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0316 - val_loss: 4.9162\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0539 - val_loss: 7.4028\n",
            "wandb: Agent Finished Run: 5zldaje8 \n",
            "\n",
            "wandb: Agent Starting Run: joqzxcuv with config:\n",
            "\tbatch_size: 3\n",
            "\tlearn: 0.01\n",
            "wandb: Agent Started Run: joqzxcuv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/joqzxcuv\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/joqzxcuv</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "19/19 [==============================] - 1s 47ms/step - loss: 0.1316 - val_loss: 5.6258\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.2179 - val_loss: 1.7402\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0496 - val_loss: 3.1510\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1826 - val_loss: 5.3884\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.3395 - val_loss: 5.5990\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 12ms/step - loss: 0.3612 - val_loss: 8.3654\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.4038 - val_loss: 2.3801\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.3468 - val_loss: 13.4954\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.7863 - val_loss: 23.7296\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.1428 - val_loss: 4.3443\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 11ms/step - loss: 0.2384 - val_loss: 3.6370\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1642 - val_loss: 5.5275\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.5248 - val_loss: 17.9812\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.1099 - val_loss: 3.7364\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.2075 - val_loss: 7.2902\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.2252 - val_loss: 4.2489\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.3631 - val_loss: 17.7304\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0814 - val_loss: 8.7137\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0916 - val_loss: 5.8009\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.2205 - val_loss: 12.2389\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0302 - val_loss: 6.4038\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0294 - val_loss: 5.4347\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0533 - val_loss: 6.8663\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0365 - val_loss: 4.0872\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0547 - val_loss: 7.3409\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0276 - val_loss: 4.4435\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0301 - val_loss: 7.4143\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 10ms/step - loss: 0.0121 - val_loss: 4.5544\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0195 - val_loss: 4.9489\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 9ms/step - loss: 0.0261 - val_loss: 3.2884\n",
            "wandb: Agent Finished Run: joqzxcuv \n",
            "\n",
            "wandb: Agent Starting Run: 1ad2ute8 with config:\n",
            "\tbatch_size: 4\n",
            "\tlearn: 0.001\n",
            "wandb: Agent Started Run: 1ad2ute8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/1ad2ute8\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/1ad2ute8</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "14/14 [==============================] - 1s 64ms/step - loss: 0.6340 - val_loss: 29.7070\n",
            "Epoch 2/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1834 - val_loss: 12.5767\n",
            "Epoch 3/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0300 - val_loss: 3.3162\n",
            "Epoch 4/30\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 0.0324 - val_loss: 2.9804\n",
            "Epoch 5/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0408 - val_loss: 3.7650\n",
            "Epoch 6/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0247 - val_loss: 2.5130\n",
            "Epoch 7/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0221 - val_loss: 3.8391\n",
            "Epoch 8/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0383 - val_loss: 2.1641\n",
            "Epoch 9/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0717 - val_loss: 6.7827\n",
            "Epoch 10/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 3.2279\n",
            "Epoch 11/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0083 - val_loss: 3.8908\n",
            "Epoch 12/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 3.8286\n",
            "Epoch 13/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0183 - val_loss: 2.6950\n",
            "Epoch 14/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0238 - val_loss: 5.1787\n",
            "Epoch 15/30\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.0077 - val_loss: 3.2271\n",
            "Epoch 16/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0104 - val_loss: 4.0635\n",
            "Epoch 17/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0095 - val_loss: 3.4213\n",
            "Epoch 18/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0145 - val_loss: 4.0551\n",
            "Epoch 19/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0132 - val_loss: 2.9625\n",
            "Epoch 20/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0098 - val_loss: 3.6949\n",
            "Epoch 21/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0184 - val_loss: 2.7141\n",
            "Epoch 22/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0080 - val_loss: 3.2943\n",
            "Epoch 23/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0097 - val_loss: 2.4514\n",
            "Epoch 24/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0143 - val_loss: 3.4241\n",
            "Epoch 25/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0163 - val_loss: 2.3714\n",
            "Epoch 26/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0536 - val_loss: 5.7220\n",
            "Epoch 27/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0388 - val_loss: 2.7832\n",
            "Epoch 28/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0975 - val_loss: 7.3954\n",
            "Epoch 29/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0287 - val_loss: 3.2278\n",
            "Epoch 30/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0119 - val_loss: 4.3324\n",
            "wandb: Agent Finished Run: 1ad2ute8 \n",
            "\n",
            "wandb: Agent Starting Run: 44tcdsuf with config:\n",
            "\tbatch_size: 4\n",
            "\tlearn: 0.0015\n",
            "wandb: Agent Started Run: 44tcdsuf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/44tcdsuf\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/44tcdsuf</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "14/14 [==============================] - 1s 65ms/step - loss: 0.6301 - val_loss: 28.9900\n",
            "Epoch 2/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0673 - val_loss: 7.2441\n",
            "Epoch 3/30\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 0.1796 - val_loss: 1.4784\n",
            "Epoch 4/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2351 - val_loss: 13.4121\n",
            "Epoch 5/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1001 - val_loss: 9.2243\n",
            "Epoch 6/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0137 - val_loss: 4.5483\n",
            "Epoch 7/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0108 - val_loss: 4.3063\n",
            "Epoch 8/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0138 - val_loss: 4.9035\n",
            "Epoch 9/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0199 - val_loss: 4.0571\n",
            "Epoch 10/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0433 - val_loss: 5.8199\n",
            "Epoch 11/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0295 - val_loss: 4.2687\n",
            "Epoch 12/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0130 - val_loss: 4.9337\n",
            "Epoch 13/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0182 - val_loss: 4.2267\n",
            "Epoch 14/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0144 - val_loss: 3.4271\n",
            "Epoch 15/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0205 - val_loss: 4.7773\n",
            "Epoch 16/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0115 - val_loss: 3.1611\n",
            "Epoch 17/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0159 - val_loss: 4.0115\n",
            "Epoch 18/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0183 - val_loss: 2.5814\n",
            "Epoch 19/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0263 - val_loss: 4.2748\n",
            "Epoch 20/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0296 - val_loss: 2.5333\n",
            "Epoch 21/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0717 - val_loss: 6.8544\n",
            "Epoch 22/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0180 - val_loss: 3.2961\n",
            "Epoch 23/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 4.2765\n",
            "Epoch 24/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 3.2183\n",
            "Epoch 25/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0188 - val_loss: 4.2253\n",
            "Epoch 26/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0243 - val_loss: 3.0353\n",
            "Epoch 27/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0421 - val_loss: 5.7802\n",
            "Epoch 28/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 3.1168\n",
            "Epoch 29/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0464 - val_loss: 6.2736\n",
            "Epoch 30/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0153 - val_loss: 3.3188\n",
            "wandb: Agent Finished Run: 44tcdsuf \n",
            "\n",
            "wandb: Agent Starting Run: 9a14qmbs with config:\n",
            "\tbatch_size: 4\n",
            "\tlearn: 0.002\n",
            "wandb: Agent Started Run: 9a14qmbs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/9a14qmbs\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/9a14qmbs</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "14/14 [==============================] - 1s 63ms/step - loss: 0.5238 - val_loss: 23.0237\n",
            "Epoch 2/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0272 - val_loss: 2.6523\n",
            "Epoch 3/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0347 - val_loss: 2.7267\n",
            "Epoch 4/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.1298 - val_loss: 3.8921\n",
            "Epoch 5/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.2060 - val_loss: 10.2127\n",
            "Epoch 6/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0842 - val_loss: 4.3372\n",
            "Epoch 7/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1869 - val_loss: 11.1025\n",
            "Epoch 8/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0215 - val_loss: 5.1067\n",
            "Epoch 9/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0218 - val_loss: 4.9600\n",
            "Epoch 10/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0114 - val_loss: 4.8367\n",
            "Epoch 11/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 4.1761\n",
            "Epoch 12/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 4.5330\n",
            "Epoch 13/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0167 - val_loss: 3.4981\n",
            "Epoch 14/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0379 - val_loss: 5.3382\n",
            "Epoch 15/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0515 - val_loss: 2.5863\n",
            "Epoch 16/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.1567 - val_loss: 10.0071\n",
            "Epoch 17/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 4.6203\n",
            "Epoch 18/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0114 - val_loss: 4.3397\n",
            "Epoch 19/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0123 - val_loss: 5.0189\n",
            "Epoch 20/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0092 - val_loss: 3.9979\n",
            "Epoch 21/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0159 - val_loss: 3.9508\n",
            "Epoch 22/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0104 - val_loss: 3.7030\n",
            "Epoch 23/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0123 - val_loss: 2.8523\n",
            "Epoch 24/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0130 - val_loss: 3.0438\n",
            "Epoch 25/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0327 - val_loss: 2.4669\n",
            "Epoch 26/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1745 - val_loss: 9.1542\n",
            "Epoch 27/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0228 - val_loss: 3.7045\n",
            "Epoch 28/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 4.4284\n",
            "Epoch 29/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 3.7395\n",
            "Epoch 30/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 3.5472\n",
            "wandb: Agent Finished Run: 9a14qmbs \n",
            "\n",
            "wandb: Agent Starting Run: wnjb4qfj with config:\n",
            "\tbatch_size: 4\n",
            "\tlearn: 0.003\n",
            "wandb: Agent Started Run: wnjb4qfj\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/wnjb4qfj\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/wnjb4qfj</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "14/14 [==============================] - 1s 63ms/step - loss: 0.3224 - val_loss: 13.2952\n",
            "Epoch 2/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.2293 - val_loss: 2.3184\n",
            "Epoch 3/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.8363 - val_loss: 34.2812\n",
            "Epoch 4/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.4122 - val_loss: 21.9319\n",
            "Epoch 5/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1091 - val_loss: 9.2573\n",
            "Epoch 6/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1577 - val_loss: 3.7195\n",
            "Epoch 7/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.4094 - val_loss: 18.9888\n",
            "Epoch 8/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1579 - val_loss: 14.1933\n",
            "Epoch 9/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0478 - val_loss: 7.8856\n",
            "Epoch 10/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0391 - val_loss: 6.5079\n",
            "Epoch 11/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0860 - val_loss: 7.7301\n",
            "Epoch 12/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1018 - val_loss: 7.3787\n",
            "Epoch 13/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0347 - val_loss: 5.8527\n",
            "Epoch 14/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 5.0194\n",
            "Epoch 15/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0180 - val_loss: 6.0024\n",
            "Epoch 16/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0259 - val_loss: 4.5689\n",
            "Epoch 17/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0568 - val_loss: 7.0260\n",
            "Epoch 18/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0586 - val_loss: 4.3075\n",
            "Epoch 19/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1289 - val_loss: 9.3624\n",
            "Epoch 20/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0202 - val_loss: 5.6049\n",
            "Epoch 21/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0102 - val_loss: 6.8757\n",
            "Epoch 22/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0194 - val_loss: 5.6145\n",
            "Epoch 23/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0139 - val_loss: 4.9082\n",
            "Epoch 24/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0239 - val_loss: 5.0833\n",
            "Epoch 25/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 3.9029\n",
            "Epoch 26/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0130 - val_loss: 3.4073\n",
            "Epoch 27/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0377 - val_loss: 5.3772\n",
            "Epoch 28/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.0464 - val_loss: 3.4365\n",
            "Epoch 29/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0987 - val_loss: 7.9388\n",
            "Epoch 30/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0169 - val_loss: 4.5912\n",
            "wandb: Agent Finished Run: wnjb4qfj \n",
            "\n",
            "wandb: Agent Starting Run: rigzo6k1 with config:\n",
            "\tbatch_size: 4\n",
            "\tlearn: 0.004\n",
            "wandb: Agent Started Run: rigzo6k1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/rigzo6k1\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/rigzo6k1</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "14/14 [==============================] - 1s 63ms/step - loss: 0.1585 - val_loss: 5.0318\n",
            "Epoch 2/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.4696 - val_loss: 2.3291\n",
            "Epoch 3/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.7984 - val_loss: 39.9806\n",
            "Epoch 4/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3939 - val_loss: 13.7674\n",
            "Epoch 5/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1354 - val_loss: 9.7261\n",
            "Epoch 6/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0651 - val_loss: 5.1419\n",
            "Epoch 7/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1231 - val_loss: 10.5837\n",
            "Epoch 8/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0389 - val_loss: 9.1886\n",
            "Epoch 9/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0250 - val_loss: 5.0802\n",
            "Epoch 10/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0425 - val_loss: 7.1563\n",
            "Epoch 11/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0436 - val_loss: 4.7115\n",
            "Epoch 12/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1269 - val_loss: 8.2922\n",
            "Epoch 13/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0503 - val_loss: 5.3892\n",
            "Epoch 14/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0551 - val_loss: 8.4082\n",
            "Epoch 15/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0251 - val_loss: 4.9211\n",
            "Epoch 16/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0389 - val_loss: 6.8108\n",
            "Epoch 17/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0286 - val_loss: 4.6094\n",
            "Epoch 18/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0463 - val_loss: 7.0712\n",
            "Epoch 19/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0441 - val_loss: 4.4274\n",
            "Epoch 20/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0868 - val_loss: 8.1309\n",
            "Epoch 21/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0224 - val_loss: 4.2665\n",
            "Epoch 22/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0146 - val_loss: 4.5393\n",
            "Epoch 23/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0124 - val_loss: 3.5094\n",
            "Epoch 24/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0157 - val_loss: 4.0606\n",
            "Epoch 25/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0225 - val_loss: 2.6725\n",
            "Epoch 26/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0615 - val_loss: 4.9801\n",
            "Epoch 27/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0550 - val_loss: 3.4362\n",
            "Epoch 28/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1666 - val_loss: 8.8694\n",
            "Epoch 29/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0392 - val_loss: 2.9637\n",
            "Epoch 30/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2187 - val_loss: 10.0740\n",
            "wandb: Agent Finished Run: rigzo6k1 \n",
            "\n",
            "wandb: Agent Starting Run: qa5kenpg with config:\n",
            "\tbatch_size: 4\n",
            "\tlearn: 0.01\n",
            "wandb: Agent Started Run: qa5kenpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/qa5kenpg\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/qa5kenpg</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "14/14 [==============================] - 1s 62ms/step - loss: 0.0997 - val_loss: 0.8689\n",
            "Epoch 2/30\n",
            "14/14 [==============================] - 0s 11ms/step - loss: 0.3539 - val_loss: 0.7217\n",
            "Epoch 3/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.8958 - val_loss: 4.4309\n",
            "Epoch 4/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.5164 - val_loss: 23.3169\n",
            "Epoch 5/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 1.4338 - val_loss: 20.6575\n",
            "Epoch 6/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.4443 - val_loss: 14.1092\n",
            "Epoch 7/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1924 - val_loss: 14.8237\n",
            "Epoch 8/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.7033 - val_loss: 22.1736\n",
            "Epoch 9/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.9352 - val_loss: 15.4552\n",
            "Epoch 10/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3341 - val_loss: 10.9442\n",
            "Epoch 11/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2339 - val_loss: 11.1174\n",
            "Epoch 12/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.1606 - val_loss: 10.1314\n",
            "Epoch 13/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1281 - val_loss: 8.5874\n",
            "Epoch 14/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.2154 - val_loss: 8.1864\n",
            "Epoch 15/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.5632 - val_loss: 22.8984\n",
            "Epoch 16/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.3053 - val_loss: 18.3191\n",
            "Epoch 17/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0731 - val_loss: 12.2691\n",
            "Epoch 18/30\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 0.0392 - val_loss: 9.7134\n",
            "Epoch 19/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0660 - val_loss: 10.5429\n",
            "Epoch 20/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0168 - val_loss: 8.6296\n",
            "Epoch 21/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0448 - val_loss: 8.7689\n",
            "Epoch 22/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0706 - val_loss: 7.1441\n",
            "Epoch 23/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.1172 - val_loss: 11.4156\n",
            "Epoch 24/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0824 - val_loss: 8.3668\n",
            "Epoch 25/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0751 - val_loss: 7.1917\n",
            "Epoch 26/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0741 - val_loss: 10.2450\n",
            "Epoch 27/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0399 - val_loss: 8.2289\n",
            "Epoch 28/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0205 - val_loss: 7.7965\n",
            "Epoch 29/30\n",
            "14/14 [==============================] - 0s 10ms/step - loss: 0.0402 - val_loss: 7.2665\n",
            "Epoch 30/30\n",
            "14/14 [==============================] - 0s 9ms/step - loss: 0.0376 - val_loss: 6.2996\n",
            "wandb: Agent Finished Run: qa5kenpg \n",
            "\n",
            "wandb: Agent Starting Run: 0nwlppde with config:\n",
            "\tbatch_size: 5\n",
            "\tlearn: 0.001\n",
            "wandb: Agent Started Run: 0nwlppde\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/0nwlppde\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/0nwlppde</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "12/12 [==============================] - 1s 71ms/step - loss: 1.4021 - val_loss: 34.4464\n",
            "Epoch 2/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5742 - val_loss: 17.2435\n",
            "Epoch 3/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0680 - val_loss: 5.5815\n",
            "Epoch 4/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1382 - val_loss: 2.7532\n",
            "Epoch 5/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0726 - val_loss: 6.7478\n",
            "Epoch 6/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0126 - val_loss: 4.6530\n",
            "Epoch 7/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0108 - val_loss: 3.5611\n",
            "Epoch 8/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0145 - val_loss: 3.3970\n",
            "Epoch 9/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0127 - val_loss: 3.9732\n",
            "Epoch 10/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0134 - val_loss: 3.5700\n",
            "Epoch 11/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0506 - val_loss: 4.5994\n",
            "Epoch 12/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0627 - val_loss: 2.9556\n",
            "Epoch 13/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0870 - val_loss: 6.8008\n",
            "Epoch 14/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0392 - val_loss: 3.7273\n",
            "Epoch 15/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.1124 - val_loss: 2.4751\n",
            "Epoch 16/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1271 - val_loss: 7.2710\n",
            "Epoch 17/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0091 - val_loss: 4.6131\n",
            "Epoch 18/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 3.6926\n",
            "Epoch 19/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0290 - val_loss: 4.8403\n",
            "Epoch 20/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0168 - val_loss: 3.7068\n",
            "Epoch 21/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0140 - val_loss: 4.3593\n",
            "Epoch 22/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0242 - val_loss: 3.7122\n",
            "Epoch 23/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0309 - val_loss: 2.9560\n",
            "Epoch 24/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0177 - val_loss: 4.3372\n",
            "Epoch 25/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0097 - val_loss: 3.7784\n",
            "Epoch 26/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0131 - val_loss: 3.5955\n",
            "Epoch 27/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0178 - val_loss: 4.6842\n",
            "Epoch 28/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0119 - val_loss: 3.8244\n",
            "Epoch 29/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0273 - val_loss: 4.1715\n",
            "Epoch 30/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0315 - val_loss: 3.1125\n",
            "wandb: Agent Finished Run: 0nwlppde \n",
            "\n",
            "wandb: Agent Starting Run: bq570vem with config:\n",
            "\tbatch_size: 5\n",
            "\tlearn: 0.0015\n",
            "wandb: Agent Started Run: bq570vem\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/bq570vem\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/bq570vem</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "12/12 [==============================] - 1s 71ms/step - loss: 1.0004 - val_loss: 25.6818\n",
            "Epoch 2/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0649 - val_loss: 5.3285\n",
            "Epoch 3/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.3269 - val_loss: 1.3598\n",
            "Epoch 4/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3570 - val_loss: 11.6963\n",
            "Epoch 5/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1385 - val_loss: 7.3525\n",
            "Epoch 6/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0343 - val_loss: 3.3573\n",
            "Epoch 7/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0429 - val_loss: 4.6116\n",
            "Epoch 8/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0814 - val_loss: 6.1680\n",
            "Epoch 9/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0195 - val_loss: 3.9160\n",
            "Epoch 10/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0137 - val_loss: 3.8825\n",
            "Epoch 11/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0105 - val_loss: 4.5020\n",
            "Epoch 12/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0169 - val_loss: 4.1216\n",
            "Epoch 13/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0284 - val_loss: 3.4120\n",
            "Epoch 14/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0469 - val_loss: 5.1138\n",
            "Epoch 15/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0298 - val_loss: 3.2533\n",
            "Epoch 16/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0400 - val_loss: 3.7783\n",
            "Epoch 17/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0282 - val_loss: 2.7192\n",
            "Epoch 18/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0303 - val_loss: 3.9729\n",
            "Epoch 19/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0231 - val_loss: 2.9190\n",
            "Epoch 20/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0350 - val_loss: 3.3556\n",
            "Epoch 21/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0257 - val_loss: 3.2187\n",
            "Epoch 22/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0263 - val_loss: 3.7259\n",
            "Epoch 23/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0277 - val_loss: 2.5366\n",
            "Epoch 24/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0301 - val_loss: 2.8634\n",
            "Epoch 25/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1109 - val_loss: 2.4789\n",
            "Epoch 26/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4071 - val_loss: 10.5618\n",
            "Epoch 27/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0501 - val_loss: 4.7563\n",
            "Epoch 28/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1264 - val_loss: 2.8695\n",
            "Epoch 29/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1226 - val_loss: 7.6381\n",
            "Epoch 30/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 4.6063\n",
            "wandb: Agent Finished Run: bq570vem \n",
            "\n",
            "wandb: Agent Starting Run: 7ad5nsm3 with config:\n",
            "\tbatch_size: 5\n",
            "\tlearn: 0.002\n",
            "wandb: Agent Started Run: 7ad5nsm3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/7ad5nsm3\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/7ad5nsm3</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "12/12 [==============================] - 1s 71ms/step - loss: 1.2642 - val_loss: 27.6543\n",
            "Epoch 2/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0301 - val_loss: 3.8069\n",
            "Epoch 3/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.3067 - val_loss: 1.5160\n",
            "Epoch 4/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6050 - val_loss: 15.1500\n",
            "Epoch 5/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1719 - val_loss: 8.6171\n",
            "Epoch 6/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0519 - val_loss: 3.7495\n",
            "Epoch 7/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0549 - val_loss: 6.1492\n",
            "Epoch 8/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0306 - val_loss: 4.7109\n",
            "Epoch 9/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0267 - val_loss: 4.4645\n",
            "Epoch 10/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0289 - val_loss: 4.8333\n",
            "Epoch 11/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0314 - val_loss: 3.4206\n",
            "Epoch 12/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0342 - val_loss: 3.1259\n",
            "Epoch 13/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0340 - val_loss: 4.3823\n",
            "Epoch 14/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0407 - val_loss: 2.7841\n",
            "Epoch 15/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0315 - val_loss: 2.8333\n",
            "Epoch 16/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0220 - val_loss: 3.0096\n",
            "Epoch 17/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0246 - val_loss: 2.4039\n",
            "Epoch 18/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0518 - val_loss: 2.2688\n",
            "Epoch 19/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1984 - val_loss: 5.9805\n",
            "Epoch 20/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1214 - val_loss: 3.0423\n",
            "Epoch 21/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2692 - val_loss: 9.2744\n",
            "Epoch 22/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0067 - val_loss: 5.0900\n",
            "Epoch 23/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0454 - val_loss: 4.7963\n",
            "Epoch 24/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0219 - val_loss: 4.2006\n",
            "Epoch 25/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0342 - val_loss: 5.5162\n",
            "Epoch 26/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0250 - val_loss: 3.8467\n",
            "Epoch 27/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0341 - val_loss: 4.6662\n",
            "Epoch 28/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0494 - val_loss: 3.4933\n",
            "Epoch 29/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0838 - val_loss: 5.7238\n",
            "Epoch 30/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0332 - val_loss: 3.1333\n",
            "wandb: Agent Finished Run: 7ad5nsm3 \n",
            "\n",
            "wandb: Agent Starting Run: osdesxgw with config:\n",
            "\tbatch_size: 5\n",
            "\tlearn: 0.003\n",
            "wandb: Agent Started Run: osdesxgw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/osdesxgw\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/osdesxgw</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "12/12 [==============================] - 1s 71ms/step - loss: 0.8769 - val_loss: 18.7156\n",
            "Epoch 2/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.2259 - val_loss: 3.5145\n",
            "Epoch 3/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.0257 - val_loss: 22.8876\n",
            "Epoch 4/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2683 - val_loss: 12.8745\n",
            "Epoch 5/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0915 - val_loss: 6.6803\n",
            "Epoch 6/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0724 - val_loss: 8.3931\n",
            "Epoch 7/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1522 - val_loss: 7.9598\n",
            "Epoch 8/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0700 - val_loss: 5.3470\n",
            "Epoch 9/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0839 - val_loss: 7.9331\n",
            "Epoch 10/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 5.4528\n",
            "Epoch 11/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0601 - val_loss: 6.9318\n",
            "Epoch 12/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0733 - val_loss: 5.0008\n",
            "Epoch 13/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0998 - val_loss: 6.3775\n",
            "Epoch 14/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0600 - val_loss: 4.1237\n",
            "Epoch 15/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1190 - val_loss: 6.6838\n",
            "Epoch 16/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0706 - val_loss: 4.2115\n",
            "Epoch 17/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0392 - val_loss: 5.7393\n",
            "Epoch 18/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0106 - val_loss: 5.1314\n",
            "Epoch 19/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0120 - val_loss: 4.9296\n",
            "Epoch 20/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0626 - val_loss: 5.1185\n",
            "Epoch 21/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0979 - val_loss: 3.4230\n",
            "Epoch 22/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1506 - val_loss: 6.6682\n",
            "Epoch 23/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0753 - val_loss: 4.2356\n",
            "Epoch 24/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0774 - val_loss: 6.8041\n",
            "Epoch 25/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0304 - val_loss: 4.8252\n",
            "Epoch 26/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0318 - val_loss: 6.1665\n",
            "Epoch 27/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0267 - val_loss: 4.9349\n",
            "Epoch 28/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0608 - val_loss: 5.6010\n",
            "Epoch 29/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0784 - val_loss: 3.8649\n",
            "Epoch 30/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0682 - val_loss: 6.7876\n",
            "wandb: Agent Finished Run: osdesxgw \n",
            "\n",
            "wandb: Agent Starting Run: 9e6id0bu with config:\n",
            "\tbatch_size: 5\n",
            "\tlearn: 0.004\n",
            "wandb: Agent Started Run: 9e6id0bu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/9e6id0bu\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/9e6id0bu</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "12/12 [==============================] - 1s 69ms/step - loss: 0.5793 - val_loss: 9.9515\n",
            "Epoch 2/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.8077 - val_loss: 4.3509\n",
            "Epoch 3/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.9880 - val_loss: 43.2244\n",
            "Epoch 4/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.0050 - val_loss: 27.0267\n",
            "Epoch 5/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3387 - val_loss: 12.8716\n",
            "Epoch 6/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2301 - val_loss: 5.1724\n",
            "Epoch 7/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3778 - val_loss: 15.3035\n",
            "Epoch 8/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.0718 - val_loss: 9.3849\n",
            "Epoch 9/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1397 - val_loss: 6.1074\n",
            "Epoch 10/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3462 - val_loss: 12.0618\n",
            "Epoch 11/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0398 - val_loss: 5.5018\n",
            "Epoch 12/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0602 - val_loss: 4.9495\n",
            "Epoch 13/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0724 - val_loss: 5.9846\n",
            "Epoch 14/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0287 - val_loss: 3.4503\n",
            "Epoch 15/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.0137 - val_loss: 4.4452\n",
            "Epoch 16/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0193 - val_loss: 4.5147\n",
            "Epoch 17/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0315 - val_loss: 3.8671\n",
            "Epoch 18/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1079 - val_loss: 5.4123\n",
            "Epoch 19/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1343 - val_loss: 3.3707\n",
            "Epoch 20/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.2202 - val_loss: 8.4454\n",
            "Epoch 21/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0473 - val_loss: 5.5182\n",
            "Epoch 22/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0327 - val_loss: 6.2963\n",
            "Epoch 23/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0259 - val_loss: 4.9371\n",
            "Epoch 24/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0289 - val_loss: 5.2681\n",
            "Epoch 25/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0302 - val_loss: 3.5439\n",
            "Epoch 26/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0933 - val_loss: 5.7392\n",
            "Epoch 27/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0624 - val_loss: 2.7467\n",
            "Epoch 28/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0619 - val_loss: 4.1674\n",
            "Epoch 29/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0530 - val_loss: 2.8999\n",
            "Epoch 30/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0280 - val_loss: 4.9766\n",
            "wandb: Agent Finished Run: 9e6id0bu \n",
            "\n",
            "wandb: Agent Starting Run: hokmq05t with config:\n",
            "\tbatch_size: 5\n",
            "\tlearn: 0.01\n",
            "wandb: Agent Started Run: hokmq05t\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignoring project='covid-forecast' passed to wandb.init when running a sweep\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/igodfried/uncategorized\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized</a><br/>\n",
              "                Sweep page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/sweeps/sz2r7l4w</a><br/>\n",
              "Run page: <a href=\"https://app.wandb.ai/igodfried/uncategorized/runs/hokmq05t\" target=\"_blank\">https://app.wandb.ai/igodfried/uncategorized/runs/hokmq05t</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "WARNING:tensorflow:Layer bidirectional is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "12/12 [==============================] - 1s 71ms/step - loss: 0.3085 - val_loss: 14.7724\n",
            "Epoch 2/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4912 - val_loss: 11.7874\n",
            "Epoch 3/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.7113 - val_loss: 17.2346\n",
            "Epoch 4/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.6644 - val_loss: 42.6668\n",
            "Epoch 5/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.3710 - val_loss: 10.3652\n",
            "Epoch 6/30\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.4866 - val_loss: 4.6774\n",
            "Epoch 7/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.7827 - val_loss: 15.0306\n",
            "Epoch 8/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4815 - val_loss: 8.8566\n",
            "Epoch 9/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.1672 - val_loss: 25.7902\n",
            "Epoch 10/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.3273 - val_loss: 12.5715\n",
            "Epoch 11/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6977 - val_loss: 11.9048\n",
            "Epoch 12/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.1074 - val_loss: 24.0079\n",
            "Epoch 13/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2565 - val_loss: 14.0797\n",
            "Epoch 14/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.1362 - val_loss: 6.7680\n",
            "Epoch 15/30\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.1610 - val_loss: 12.9133\n",
            "Epoch 16/30\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.0589 - val_loss: 7.5791\n",
            "Epoch 17/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0846 - val_loss: 7.2938\n",
            "Epoch 18/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0516 - val_loss: 5.9777\n",
            "Epoch 19/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0872 - val_loss: 6.9130\n",
            "Epoch 20/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.1659 - val_loss: 5.7129\n",
            "Epoch 21/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1632 - val_loss: 10.7843\n",
            "Epoch 22/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0351 - val_loss: 8.4132\n",
            "Epoch 23/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0283 - val_loss: 9.6677\n",
            "Epoch 24/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0339 - val_loss: 8.9216\n",
            "Epoch 25/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0352 - val_loss: 8.0100\n",
            "Epoch 26/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0224 - val_loss: 7.8943\n",
            "Epoch 27/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0161 - val_loss: 6.6905\n",
            "Epoch 28/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0414 - val_loss: 6.4662\n",
            "Epoch 29/30\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0234 - val_loss: 5.3942\n",
            "Epoch 30/30\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.0105 - val_loss: 6.3792\n",
            "wandb: Agent Finished Run: hokmq05t \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR_vweL-bOD1",
        "colab_type": "code",
        "outputId": "f6d40e8b-d959-48ff-85d9-d828cd00779b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.legend();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 370,
              "height": 248
            },
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPaMFelHjm-m",
        "colab_type": "text"
      },
      "source": [
        "### Examining Results\n",
        "We will now predict both one step ahead and 20 steps ahead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5zjK1JYf52W",
        "colab_type": "code",
        "outputId": "00fa37da-16a3-428d-bb6c-e6f10371bb94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "res = model.predict(X_test)\n",
        "res = r_test.inverse_transform(res)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.1279370e-02,  2.0092883e+03],\n",
              "       [-1.8879963e-02,  2.2472920e+03],\n",
              "       [-1.6375745e-02,  2.5275439e+03],\n",
              "       [-1.3892809e-02,  2.8506108e+03],\n",
              "       [-1.1596879e-02,  3.1829221e+03],\n",
              "       [-9.0861954e-03,  3.5029392e+03],\n",
              "       [-6.4477865e-03,  3.7825161e+03],\n",
              "       [-3.6051013e-03,  4.0421055e+03],\n",
              "       [-1.2287628e-03,  4.2927124e+03],\n",
              "       [ 3.3153594e-04,  4.5448154e+03],\n",
              "       [ 1.1798348e-03,  4.7915127e+03],\n",
              "       [ 2.0689592e-03,  5.0360225e+03],\n",
              "       [ 3.3188127e-03,  5.2662153e+03],\n",
              "       [ 5.0343592e-03,  5.5053794e+03],\n",
              "       [ 6.5031778e-03,  5.7533242e+03],\n",
              "       [ 7.2021466e-03,  5.9751943e+03],\n",
              "       [ 7.6369299e-03,  6.1531494e+03]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_9dG21n1bJC",
        "colab_type": "code",
        "outputId": "aee9ba88-9e6e-4ec8-ad2c-a114d299538a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "y_true = r_test.inverse_transform(y_test)\n",
        "y_true"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0., 3521.],\n",
              "       [   0., 3822.],\n",
              "       [   0., 4086.],\n",
              "       [   0., 4179.],\n",
              "       [   0., 4265.],\n",
              "       [   0., 4330.],\n",
              "       [   0., 4470.],\n",
              "       [   0., 4645.],\n",
              "       [   0., 4855.],\n",
              "       [   0., 4965.],\n",
              "       [   0., 5028.],\n",
              "       [   0., 5079.],\n",
              "       [   0., 5241.],\n",
              "       [   0., 5415.],\n",
              "       [   0., 5449.],\n",
              "       [   0., 5449.],\n",
              "       [   0., 5449.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVKR-Hw_1lQ8",
        "colab_type": "code",
        "outputId": "7e86523f-421b-47f5-a08a-8c5aaaa057b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "def predict_multi(model, time_steps, start_rows):\n",
        "  start_rows=np.expand_dims(start_rows, axis=0)\n",
        "  for i in range(0, time_steps):\n",
        "    out = model.predict(start_rows[:, i:, :])\n",
        "    out = out[np.newaxis, ...]\n",
        "    start_rows = np.concatenate((start_rows, out), axis=1)\n",
        "  return start_rows[:, config[\"seq_len\"]:, :]\n",
        "\n",
        "arr = predict_multi(model, len(test)-config[\"seq_len\"], X_test[0, :, :])\n",
        "test_orig['predicted_cases'] = 0\n",
        "test_orig['predicted_cases'][config[\"seq_len\"]:] = r_test.inverse_transform(arr.squeeze(0))[:, 1]\n",
        "plt.plot(test_orig['predicted_cases'], label='predicted_cases')\n",
        "plt.plot(test_orig['cases'], label='actual_cases')\n",
        "plt.legend();\n",
        "wandb.log({\"test\":plt})"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning:\n",
            "\n",
            "\n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/plotly/matplotlylib/mpltools.py:368: MatplotlibDeprecationWarning:\n",
            "\n",
            "\n",
            "The is_frame_like function was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/plotly/matplotlylib/renderer.py:410: UserWarning:\n",
            "\n",
            "Bummer! Plotly can currently only draw Line2D objects from matplotlib that are in 'data' coordinates!\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/plotly/matplotlylib/renderer.py:512: UserWarning:\n",
            "\n",
            "I found a path object that I don't think is part of a bar chart. Ignoring.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoIE9nyy3gVB",
        "colab_type": "code",
        "outputId": "fccf7eb0-be54-446c-f780-611b7b7d1e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "r_test.inverse_transform(X_test[0, :, :])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0., 2342.],\n",
              "       [   0., 2508.],\n",
              "       [   0., 2694.],\n",
              "       [   0., 2802.],\n",
              "       [   0., 2881.],\n",
              "       [   0., 3128.],\n",
              "       [   0., 3307.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW4Nwm9yWPia",
        "colab_type": "code",
        "outputId": "76a383f0-a538-4098-9806-a3f3806f353b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "y_multi = r_test.inverse_transform(arr.squeeze(0))\n",
        "y_multi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.12793704e-02,  2.00928912e+03],\n",
              "       [-2.36456450e-02,  1.96579090e+03],\n",
              "       [-2.32349541e-02,  1.79785547e+03],\n",
              "       [-2.29426380e-02,  1.55205121e+03],\n",
              "       [-2.49109063e-02,  1.25682677e+03],\n",
              "       [-3.02774105e-02,  9.26331511e+02],\n",
              "       [-3.81791964e-02,  5.71013687e+02],\n",
              "       [-4.35400605e-02,  2.24849555e+02],\n",
              "       [-4.84175906e-02, -5.02670822e+01],\n",
              "       [-5.37138134e-02, -3.33807449e+02],\n",
              "       [-5.92226684e-02, -6.05690453e+02],\n",
              "       [-6.46216497e-02, -8.56124453e+02],\n",
              "       [-6.95688501e-02, -1.08077419e+03],\n",
              "       [-7.38583505e-02, -1.27827176e+03],\n",
              "       [-7.75195137e-02, -1.44942704e+03],\n",
              "       [-8.07663053e-02, -1.59675538e+03],\n",
              "       [-8.34889859e-02, -1.72313082e+03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x_QvIsutf1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "x_test = y_true[:, 1] \n",
        "wandb.run.summary[\"test_mse\"] = tf.keras.losses.MSE(\n",
        "    x_test, res[:, 1]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BiFffVB3Jc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.run.summary[\"test_mse_full\"] = tf.keras.losses.MSE(\n",
        "    y_multi[:, 1], x_test\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkob-4bnYdeT",
        "colab_type": "text"
      },
      "source": [
        "### PyTorch models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGXjVazoJ9i3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import math\n",
        "from torch.nn.modules import Transformer, TransformerEncoder, TransformerDecoder, TransformerDecoderLayer, TransformerEncoderLayer, LayerNorm\n",
        "class CustomTransformerDecoder(torch.nn.Module):\n",
        "    def __init__(self, seq_length, output_seq_length, n_time_series, d_model=128, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.dense_shape = torch.nn.Linear(n_time_series, d_model)\n",
        "        self.pe = SimplePositionalEncoding(d_model)\n",
        "        encoder_layer = TransformerEncoderLayer(d_model, 8)\n",
        "        encoder_norm = LayerNorm(d_model)\n",
        "        self.transformer_enc = TransformerEncoder(encoder_layer, 6, encoder_norm)\n",
        "        self.output_dim_layer = torch.nn.Linear(d_model, output_dim)\n",
        "        self.output_seq_length = output_seq_length\n",
        "        self.out_length_lay  = torch.nn.Linear(seq_length, output_seq_length)\n",
        "        self.mask = generate_square_subsequent_mask(seq_length)\n",
        "    def forward(self, x):\n",
        "        \"\"\"\"\"\"\n",
        "        x = self.dense_shape(x)\n",
        "        x = self.pe(x)\n",
        "        x = x.permute(1,0,2)\n",
        "        x = self.transformer_enc(x, mask=self.mask)\n",
        "        x = self.output_dim_layer(x)\n",
        "        x = x.permute(1, 2, 0)\n",
        "        x = self.out_length_lay(x)\n",
        "        return x.view(-1, self.output_seq_length)\n",
        "    \n",
        "class SimplePositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(SimplePositionalEncoding, self).__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x:torch.Tensor)->torch.Tensor:\n",
        "        \"\"\"Creates a basic positional encoding\"\"\"\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "        \n",
        "def generate_square_subsequent_mask(sz:int)->torch.Tensor:\n",
        "        r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n",
        "            Unmasked positions are filled with float(0.0).\n",
        "        \"\"\"\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK-U7-QhfP5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = CustomTransformerDecoder(50, 1, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtZbZ55tfe0i",
        "colab_type": "code",
        "outputId": "8e42b01b-303a-4fbf-f55e-217198c364e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "c(torch.rand(2, 50, 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2974],\n",
              "        [-0.2575]], grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mFCWLbkktra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTMForecast(torch.nn.Module):\n",
        "  def __init__(self, seq_length: int, n_time_series: int, output_seq_len=1, hidden_states=20, num_layers=2, bias=True):\n",
        "    super().__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.forecast_history = seq_length\n",
        "    self.n_time_series = n_time_series\n",
        "    self.hidden_dim = hidden_states\n",
        "    self.lstm = torch.nn.LSTM(n_time_series, hidden_states, num_layers, bias, batch_first=True)\n",
        "    self.final_layer = torch.nn.Linear(seq_length*hidden_states, output_seq_len)\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    # even with batch_first = True this remains same as docs\n",
        "    hidden_state = torch.zeros(self.num_layers,batch_size,self.hidden_dim)\n",
        "    cell_state = torch.zeros(self.num_layers,batch_size,self.hidden_dim)\n",
        "    self.hidden = (hidden_state, cell_state)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    print(x.size()[0])\n",
        "    batch_size = x.size()[0]\n",
        "    out_x,self.hidden = self.lstm(x, self.hidden)\n",
        "    x = self.final_layer(out_x.contiguous().view(batch_size, -1))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}